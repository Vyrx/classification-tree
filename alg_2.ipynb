{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import queue\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Github\\intro-ml-project\\alg_2.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/intro-ml-project/alg_2.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_file_y_loc \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata_2\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDataset2_train\u001b[39m\u001b[39m\\\u001b[39m\u001b[39my_train.xlsx\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/intro-ml-project/alg_2.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test_file_loc \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata_2\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDataset2_test\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mX_test.xlsx\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Github/intro-ml-project/alg_2.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_data_x \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(train_file_x_loc)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/intro-ml-project/alg_2.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_data_y \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_excel(train_file_y_loc)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Github/intro-ml-project/alg_2.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m test_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_excel(test_file_loc)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:299\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    295\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStarting with Pandas version \u001b[39m\u001b[39m{\u001b[39;00mversion\u001b[39m}\u001b[39;00m\u001b[39m all arguments of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    296\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00marguments\u001b[39m}\u001b[39;00m\u001b[39m will be keyword-only\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n\u001b[0;32m    298\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39mstacklevel)\n\u001b[1;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\excel\\_base.py:344\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    339\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m     )\n\u001b[0;32m    343\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m     data \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mparse(\n\u001b[0;32m    345\u001b[0m         sheet_name\u001b[39m=\u001b[39;49msheet_name,\n\u001b[0;32m    346\u001b[0m         header\u001b[39m=\u001b[39;49mheader,\n\u001b[0;32m    347\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    348\u001b[0m         index_col\u001b[39m=\u001b[39;49mindex_col,\n\u001b[0;32m    349\u001b[0m         usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[0;32m    350\u001b[0m         squeeze\u001b[39m=\u001b[39;49msqueeze,\n\u001b[0;32m    351\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    352\u001b[0m         converters\u001b[39m=\u001b[39;49mconverters,\n\u001b[0;32m    353\u001b[0m         true_values\u001b[39m=\u001b[39;49mtrue_values,\n\u001b[0;32m    354\u001b[0m         false_values\u001b[39m=\u001b[39;49mfalse_values,\n\u001b[0;32m    355\u001b[0m         skiprows\u001b[39m=\u001b[39;49mskiprows,\n\u001b[0;32m    356\u001b[0m         nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[0;32m    357\u001b[0m         na_values\u001b[39m=\u001b[39;49mna_values,\n\u001b[0;32m    358\u001b[0m         keep_default_na\u001b[39m=\u001b[39;49mkeep_default_na,\n\u001b[0;32m    359\u001b[0m         na_filter\u001b[39m=\u001b[39;49mna_filter,\n\u001b[0;32m    360\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    361\u001b[0m         parse_dates\u001b[39m=\u001b[39;49mparse_dates,\n\u001b[0;32m    362\u001b[0m         date_parser\u001b[39m=\u001b[39;49mdate_parser,\n\u001b[0;32m    363\u001b[0m         thousands\u001b[39m=\u001b[39;49mthousands,\n\u001b[0;32m    364\u001b[0m         comment\u001b[39m=\u001b[39;49mcomment,\n\u001b[0;32m    365\u001b[0m         skipfooter\u001b[39m=\u001b[39;49mskipfooter,\n\u001b[0;32m    366\u001b[0m         convert_float\u001b[39m=\u001b[39;49mconvert_float,\n\u001b[0;32m    367\u001b[0m         mangle_dupe_cols\u001b[39m=\u001b[39;49mmangle_dupe_cols,\n\u001b[0;32m    368\u001b[0m     )\n\u001b[0;32m    369\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     \u001b[39m# make sure to close opened file handles\u001b[39;00m\n\u001b[0;32m    371\u001b[0m     \u001b[39mif\u001b[39;00m should_close:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1170\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, squeeze, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\n\u001b[0;32m   1137\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1138\u001b[0m     sheet_name\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1157\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds,\n\u001b[0;32m   1158\u001b[0m ):\n\u001b[0;32m   1159\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[39m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[39m        DataFrame from the passed in Excel file.\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1170\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mparse(\n\u001b[0;32m   1171\u001b[0m         sheet_name\u001b[39m=\u001b[39msheet_name,\n\u001b[0;32m   1172\u001b[0m         header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   1173\u001b[0m         names\u001b[39m=\u001b[39mnames,\n\u001b[0;32m   1174\u001b[0m         index_col\u001b[39m=\u001b[39mindex_col,\n\u001b[0;32m   1175\u001b[0m         usecols\u001b[39m=\u001b[39musecols,\n\u001b[0;32m   1176\u001b[0m         squeeze\u001b[39m=\u001b[39msqueeze,\n\u001b[0;32m   1177\u001b[0m         converters\u001b[39m=\u001b[39mconverters,\n\u001b[0;32m   1178\u001b[0m         true_values\u001b[39m=\u001b[39mtrue_values,\n\u001b[0;32m   1179\u001b[0m         false_values\u001b[39m=\u001b[39mfalse_values,\n\u001b[0;32m   1180\u001b[0m         skiprows\u001b[39m=\u001b[39mskiprows,\n\u001b[0;32m   1181\u001b[0m         nrows\u001b[39m=\u001b[39mnrows,\n\u001b[0;32m   1182\u001b[0m         na_values\u001b[39m=\u001b[39mna_values,\n\u001b[0;32m   1183\u001b[0m         parse_dates\u001b[39m=\u001b[39mparse_dates,\n\u001b[0;32m   1184\u001b[0m         date_parser\u001b[39m=\u001b[39mdate_parser,\n\u001b[0;32m   1185\u001b[0m         thousands\u001b[39m=\u001b[39mthousands,\n\u001b[0;32m   1186\u001b[0m         comment\u001b[39m=\u001b[39mcomment,\n\u001b[0;32m   1187\u001b[0m         skipfooter\u001b[39m=\u001b[39mskipfooter,\n\u001b[0;32m   1188\u001b[0m         convert_float\u001b[39m=\u001b[39mconvert_float,\n\u001b[0;32m   1189\u001b[0m         mangle_dupe_cols\u001b[39m=\u001b[39mmangle_dupe_cols,\n\u001b[0;32m   1190\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds,\n\u001b[0;32m   1191\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\excel\\_base.py:492\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[1;34m(self, sheet_name, header, names, index_col, usecols, squeeze, dtype, true_values, false_values, skiprows, nrows, na_values, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# assume an integer if not a string\u001b[39;00m\n\u001b[0;32m    490\u001b[0m     sheet \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_sheet_by_index(asheetname)\n\u001b[1;32m--> 492\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_sheet_data(sheet, convert_float)\n\u001b[0;32m    493\u001b[0m usecols \u001b[39m=\u001b[39m maybe_convert_usecols(usecols)\n\u001b[0;32m    495\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:548\u001b[0m, in \u001b[0;36mOpenpyxlReader.get_sheet_data\u001b[1;34m(self, sheet, convert_float)\u001b[0m\n\u001b[0;32m    546\u001b[0m data: List[List[Scalar]] \u001b[39m=\u001b[39m []\n\u001b[0;32m    547\u001b[0m last_row_with_data \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m--> 548\u001b[0m \u001b[39mfor\u001b[39;00m row_number, row \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sheet\u001b[39m.\u001b[39mrows):\n\u001b[0;32m    549\u001b[0m     converted_row \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_cell(cell, convert_float) \u001b[39mfor\u001b[39;00m cell \u001b[39min\u001b[39;00m row]\n\u001b[0;32m    550\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(cell \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m cell \u001b[39min\u001b[39;00m converted_row):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\openpyxl\\worksheet\\_read_only.py:79\u001b[0m, in \u001b[0;36mReadOnlyWorksheet._cells_by_row\u001b[1;34m(self, min_col, min_row, max_col, max_row, values_only)\u001b[0m\n\u001b[0;32m     75\u001b[0m src \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_source()\n\u001b[0;32m     76\u001b[0m parser \u001b[39m=\u001b[39m WorkSheetParser(src, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shared_strings,\n\u001b[0;32m     77\u001b[0m                          data_only\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mdata_only, epoch\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mepoch,\n\u001b[0;32m     78\u001b[0m                          date_formats\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39m_date_formats)\n\u001b[1;32m---> 79\u001b[0m \u001b[39mfor\u001b[39;00m idx, row \u001b[39min\u001b[39;00m parser\u001b[39m.\u001b[39mparse():\n\u001b[0;32m     80\u001b[0m     \u001b[39mif\u001b[39;00m max_row \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m idx \u001b[39m>\u001b[39m max_row:\n\u001b[0;32m     81\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\openpyxl\\worksheet\\_reader.py:144\u001b[0m, in \u001b[0;36mWorkSheetParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m properties \u001b[39m=\u001b[39m {\n\u001b[0;32m    126\u001b[0m     PRINT_TAG: (\u001b[39m'\u001b[39m\u001b[39mprint_options\u001b[39m\u001b[39m'\u001b[39m, PrintOptions),\n\u001b[0;32m    127\u001b[0m     MARGINS_TAG: (\u001b[39m'\u001b[39m\u001b[39mpage_margins\u001b[39m\u001b[39m'\u001b[39m, PageMargins),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m }\n\u001b[0;32m    142\u001b[0m it \u001b[39m=\u001b[39m iterparse(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource) \u001b[39m# add a finaliser to close the source when this becomes possible\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m \u001b[39mfor\u001b[39;00m _, element \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    145\u001b[0m     tag_name \u001b[39m=\u001b[39m element\u001b[39m.\u001b[39mtag\n\u001b[0;32m    146\u001b[0m     \u001b[39mif\u001b[39;00m tag_name \u001b[39min\u001b[39;00m dispatcher:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\xml\\etree\\ElementTree.py:1254\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1254\u001b[0m         \u001b[39myield from\u001b[39;00m pullparser\u001b[39m.\u001b[39mread_events()\n\u001b[0;32m   1255\u001b[0m         \u001b[39m# load event buffer\u001b[39;00m\n\u001b[0;32m   1256\u001b[0m         data \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39mread(\u001b[39m16\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\xml\\etree\\ElementTree.py:1331\u001b[0m, in \u001b[0;36mXMLPullParser.read_events\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1329\u001b[0m     \u001b[39mraise\u001b[39;00m event\n\u001b[0;32m   1330\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1331\u001b[0m     \u001b[39myield\u001b[39;00m event\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1664\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydev_bundle\\pydev_is_thread_alive.py:10\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_thread_alive\u001b[39m(t):\n\u001b[1;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39;49m_is_stopped\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_file_x_loc = 'data_2\\\\Dataset2_train\\X_train.xlsx'\n",
    "train_file_y_loc = 'data_2\\\\Dataset2_train\\y_train.xlsx'\n",
    "test_file_loc = 'data_2\\Dataset2_test\\X_test.xlsx'\n",
    "\n",
    "train_data_x = pd.read_excel(train_file_x_loc)\n",
    "train_data_y = pd.read_excel(train_file_y_loc)\n",
    "test_data = pd.read_excel(test_file_loc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_x = train_data_x.values\n",
    "tr_y = train_data_y.values\n",
    "test = test_data.values\n",
    "\n",
    "tr_x = tr_x.flatten()\n",
    "tr_y = tr_y.flatten()\n",
    "test = test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\bs4\\__init__.py:337: MarkupResemblesLocatorWarning: \".\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Python39\\lib\\site-packages\\bs4\\__init__.py:337: MarkupResemblesLocatorWarning: \"...\" looks like a directory name, not markup. You may want to open a file found in this directory and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\Python39\\lib\\site-packages\\bs4\\__init__.py:346: MarkupResemblesLocatorWarning: \"con\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "100%|██████████| 124848/124848 [02:54<00:00, 715.90it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "for raw in tqdm(train_data_x['Phrase']):\n",
    "    text = BeautifulSoup(raw).get_text()\n",
    "    only_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    words = word_tokenize(only_text.lower())\n",
    "    stops = set(stopwords.words('english'))\n",
    "    non_stopwords = [word for word in words if not word in stops]\n",
    "    lemma_words = [lemmatizer.lemmatize(word) for word in non_stopwords]    \n",
    "    reviews.append(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124848/124848 [00:00<00:00, 264897.51it/s]\n"
     ]
    }
   ],
   "source": [
    "unique_words = set()\n",
    "len_max = 0\n",
    "for sent in tqdm(reviews):\n",
    "    unique_words.update(sent)\n",
    "    if len_max < len(sent):\n",
    "        len_max = len(sent)\n",
    "len(list(unique_words)), len_max\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "tokenizer.fit_on_texts(list(reviews))\n",
    "    \n",
    "X_train = tokenizer.texts_to_sequences(reviews)\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy calculating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124848\n"
     ]
    }
   ],
   "source": [
    "labels = [0,1,2,3,4]\n",
    "\n",
    "count = np.bincount(tr_y)\n",
    "\n",
    "# Take bin count of each label value as input\n",
    "def get_entropy(count):\n",
    "    num_data = np.sum(count)\n",
    "    total = 0\n",
    "\n",
    "    for i in count:\n",
    "        prob = i / num_data\n",
    "        if prob == 0:\n",
    "            continue\n",
    "        total += prob * np.log2(prob)\n",
    "\n",
    "    return -total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.data = None    # Feature values, including labels\n",
    "        self.f_in = None    # The index of the traversal criteria\n",
    "        self.thresh = None  # The value threshold of the traversal criteria\n",
    "        self.label = None   # Label with the max amount\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.data = data\n",
    "        self.f_in = None\n",
    "        self.thresh = None\n",
    "        self.label = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124848\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Github\\intro-ml-project\\alg_2.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/intro-ml-project/alg_2.ipynb#X36sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m label2 \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/intro-ml-project/alg_2.ipynb#X36sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_dat \u001b[39min\u001b[39;00m cur_data:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Github/intro-ml-project/alg_2.ipynb#X36sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39mif\u001b[39;00m temp_dat[i] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m cur_thresh:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/intro-ml-project/alg_2.ipynb#X36sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m         label1\u001b[39m.\u001b[39mappend(\u001b[39mint\u001b[39m(temp_dat[num_features])) \u001b[39m# Append the label\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Github/intro-ml-project/alg_2.ipynb#X36sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cur_data = X_train\n",
    "num_features = len(X_train[0])\n",
    "temp = []\n",
    "\n",
    "# Append target feature to cur_data\n",
    "for i in range(len(cur_data)):\n",
    "    temp.append( np.append(cur_data[i],int(tr_y[i])) )\n",
    "cur_data = np.array(temp)\n",
    "\n",
    "root = Node(cur_data)\n",
    "cur_node = root\n",
    "\n",
    "q = queue.Queue() # A queue for processing nodes of the decision tree\n",
    "q.put(cur_node)\n",
    "\n",
    "\n",
    "# # # LOOP\n",
    "\n",
    "while q.empty() == False:\n",
    "\n",
    "    cur_node = q.get()\n",
    "    cur_data = cur_node.data\n",
    "\n",
    "    print(len(cur_data))\n",
    "\n",
    "    # If data length is 0, continue\n",
    "    if len(cur_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    cur_label = [int(temp) for temp in cur_data[:,num_features]]\n",
    "    temp_count = np.bincount(cur_label)\n",
    "\n",
    "    # Assign the selected label to the current node\n",
    "    cur_node.label = np.argmax(temp_count)\n",
    "\n",
    "    # If all data has the same label, stop\n",
    "    areAllLabelSame = False\n",
    "\n",
    "    for temp in temp_count:\n",
    "        if temp == sum(temp_count):\n",
    "            areAllLabelSame = True\n",
    "            break\n",
    "    if areAllLabelSame:\n",
    "        continue\n",
    "\n",
    "    # We want to get (feature, val) that minimizes IG\n",
    "    split_feat_in = 0 \n",
    "    split_thresh = 0\n",
    "    min_ent = np.Inf\n",
    "\n",
    "    # Go through each feature, sort the values\n",
    "    # i = current feature index\n",
    "    for i in range(num_features):  # range(num_features)\n",
    "        cur_feat_val = np.sort(cur_data[:,i])\n",
    "\n",
    "        # Iterate through each sorted value \n",
    "        for j in range(len(cur_feat_val) - 1):\n",
    "\n",
    "            # Get every unique feature values\n",
    "            if (cur_feat_val[j] !=  cur_feat_val[j+1]):\n",
    "                cur_thresh = cur_feat_val[j]\n",
    "\n",
    "                # Split data into two based on cur_thresh, get two labels\n",
    "                label1 = []\n",
    "                label2 = []\n",
    "                for temp_dat in cur_data:\n",
    "                    if temp_dat[i] <= cur_thresh:\n",
    "                        label1.append(int(temp_dat[num_features])) # Append the label\n",
    "                    else:\n",
    "                        label2.append(int(temp_dat[num_features]))\n",
    "                \n",
    "                label1 = np.array(label1)\n",
    "                label2 = np.array(label2)\n",
    "\n",
    "                count1 = np.bincount(label1)\n",
    "                count2 = np.bincount(label2)\n",
    "\n",
    "                num_labels_1 = np.sum(count1)\n",
    "                num_labels_2 = np.sum(count2)\n",
    "\n",
    "                w_avg_ent = (num_labels_1 / (num_labels_1 + num_labels_2)) * get_entropy(count1) + (num_labels_2 / (num_labels_1 + num_labels_2)) * get_entropy(count2)\n",
    "                \n",
    "                # Maximizing IG == minimizing w_avg_ent\n",
    "\n",
    "                if w_avg_ent < min_ent:\n",
    "                    min_ent = w_avg_ent\n",
    "                    split_feat_in = i\n",
    "                    split_thresh = cur_thresh\n",
    "    \n",
    "    # Split data into two\n",
    "    data_left = []\n",
    "    data_right = []\n",
    "\n",
    "    # For each data put in either left / right\n",
    "    for temp_dat in cur_data:\n",
    "        if temp_dat[split_feat_in] <= split_thresh:\n",
    "            data_left.append(temp_dat)\n",
    "        else:\n",
    "            data_right.append(temp_dat)\n",
    "    \n",
    "    data_left = np.array(data_left)\n",
    "    data_right = np.array(data_right)\n",
    "\n",
    "\n",
    "    # Update data structure\n",
    "    cur_node.thresh = split_thresh\n",
    "    cur_node.f_in = split_feat_in\n",
    "\n",
    "    cur_node.left = Node(data_left)\n",
    "    cur_node.right = Node(data_right)\n",
    "\n",
    "    q.put(cur_node.left)\n",
    "    q.put(cur_node.right)\n",
    "\n",
    "    #print(cur_node.thresh)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = []\n",
    "for raw in tqdm(test['Phrase']):\n",
    "    text = BeautifulSoup(raw).get_text()\n",
    "    only_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    words = word_tokenize(only_text.lower())\n",
    "    stops = set(stopwords.words('english'))\n",
    "    non_stopwords = [word for word in words if not word in stops]\n",
    "    lemma_words = [lemmatizer.lemmatize(word) for word in non_stopwords]    \n",
    "    test_reviews.append(lemma_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
